{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import nltk.stem\n",
    "nltk.download('rslp')\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "mlflow.set_tracking_uri(\"http://mlflow_server:5000\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mlflow.set_experiment(\"Doc2Vec-Decision-Tree\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#importing dataset\n",
    "df_train = pd.read_csv(str( 'sample_products.csv'),sep=',')\n",
    "df_test = pd.read_csv(str( 'test_products.csv'), sep=',')\n",
    "\n",
    "mlflow.sklearn.autolog(log_models=True,log_model_signatures=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1o passo Removal of Stop Words\n",
    "2o passo Tokenization\n",
    "3o passo Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatening title and tags\n",
    "df_copy = df_train.copy()\n",
    "df_copy[\"text\"] = df_copy[\"concatenated_tags\"] + \" \" + df_copy[\"query\"]+ \" \" + df_copy[\"title\"]\n",
    "df_copy = df_copy[df_copy[\"concatenated_tags\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['text']] \n",
    "print(df_copy['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Stop Words\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df_copy['tokens'] = df_copy['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "print(df_copy['tokens'].head(10))\n",
    "\n",
    "\n",
    "# tokenization\n",
    "\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['tokens']] \n",
    "print(df_copy['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stemming \n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "# Get the stemmed_tokens\n",
    "df_copy['stemmed_tokens'] = [[stemmer.stem(word) for word in tokens] for tokens in df_copy['tokenized_text']]\n",
    "df_copy['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building dictionaries\n",
    "\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df_copy['stemmed_tokens'])\n",
    "print(\"Total unique words:\")\n",
    "print(len(mydict.token2id))\n",
    "print(\"\\nSample data from dictionary:\")\n",
    "i = 0\n",
    "# Print top 4 (word, id) tuples\n",
    "for key in mydict.token2id.keys():\n",
    "    print(\"Word: {} - ID: {} \".format(key, mydict.token2id[key]))\n",
    "    if i == 3:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Bow Vectors\n",
    "vocab_len = len(mydict)\n",
    "print(\"Example of how the BOW words\")\n",
    "arr = []\n",
    "for line in df_copy['stemmed_tokens']:\n",
    "    print(\"Doc2Bow Line:\")\n",
    "    print(mydict.doc2bow(line))\n",
    "    for word in line:\n",
    "        arr.append(mydict.token2id[word])\n",
    "    print(\"Actual line:\")\n",
    "    print(line)\n",
    "    print(\"(Word, count) Tuples:\")\n",
    "    print([(mydict[id], count) for id, count in mydict.doc2bow(line) ])\n",
    "    print(\"Sparse bow vector for the line\")\n",
    "    print(gensim.matutils.corpus2csc([mydict.doc2bow(line)],num_terms=vocab_len).toarray()[:,0])\n",
    "    break\n",
    "print(\"Sorted word id list\")\n",
    "print(sorted(arr))\n",
    "\n",
    "df_copy = df_copy.fillna(0)\n",
    "\n",
    "print(df_copy.info())\n",
    "\n",
    "#Create column for each category\n",
    "df_one = pd.get_dummies(df_copy.category)\n",
    "print(df_one.head())\n",
    "df_copy = pd.concat([df_copy, df_one], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split Function\n",
    "top_data_df_small = df_copy\n",
    "def split_train_test(top_data_df_small,category, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['product_id', 'seller_id','search_page','position', 'creation_date', 'price','weight','express_delivery','minimum_quantity','view_counts','order_counts', 'stemmed_tokens']], \n",
    "                                                        top_data_df_small[category], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train set\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test set\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small,category='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TaggedDocuments are tuple of stemmed_tokens and class lable, example is printed (scroll to the right to see label)\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(top_data_df_small['stemmed_tokens'])]\n",
    "print(documents[1])\n",
    "\n",
    "# Initialize the model\n",
    "doc2vec_model = Doc2Vec(documents, vector_size=vocab_len, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Sample vector for the stemmed tokens\n",
    "vector = doc2vec_model.infer_vector(top_data_df_small['stemmed_tokens'][0])\n",
    "# Printing sample vector\n",
    "print(len(vector))\n",
    "print(\"Top 10 values in Doc2Vec inferred vector:\")\n",
    "print(vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the vectors for train data in \n",
    "doc2vec_filename =  'train_review_doc2vec.csv'\n",
    "with open(doc2vec_filename, 'w+') as doc2vec_file:\n",
    "    for index, row in X_train.iterrows():\n",
    "        model_vector = doc2vec_model.infer_vector(row['stemmed_tokens'])\n",
    "        if index == 0:\n",
    "            header = \",\".join(str(ele) for ele in range(vocab_len))\n",
    "            doc2vec_file.write(header)\n",
    "            doc2vec_file.write(\"\\n\")\n",
    "        line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        doc2vec_file.write(line1)\n",
    "        doc2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from the filename\n",
    "doc2vec_df = pd.read_csv(doc2vec_filename)\n",
    "\n",
    "clf_decision_doc2vec = DecisionTreeClassifier()\n",
    "\n",
    "categories = df_copy['category'].unique().tolist()\n",
    "\n",
    "mydict.save_as_text(\"my_dict_Doc2Vec.txt\", sort_by_word=True)\n",
    "\n",
    "# Fit the models\n",
    "for category in categories:\n",
    "    X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small,category=category)\n",
    "    with mlflow.start_run(run_name='Doc2Vec_categorizer_'+category) as run:\n",
    "        clf_decision_doc2vec.fit(doc2vec_df, Y_train[category])\n",
    "        #print(\"Logged data and model in run {}\".format(run.info.run_id))\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=clf_decision_doc2vec,\n",
    "            artifact_path=\"sklearn-model\",\n",
    "            registered_model_name=\"Doc2Vec-DecisionTreeClass-\"+category\n",
    "        )\n",
    "        mlflow.log_artifact(\"my_dict_Doc2Vec.txt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
