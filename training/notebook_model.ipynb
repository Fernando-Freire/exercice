{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.1 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing dataset\n",
    "df_train = pd.read_csv(str( 'sample_products.csv'),sep=',')\n",
    "df_test = pd.read_csv(str( 'test_products.csv'), sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1o passo Removal of Stop Words\n",
    "2o passo Tokenization\n",
    "3o passo Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatening title and tags\n",
    "df_copy = df_train.copy()\n",
    "df_copy[\"text\"] = df_copy[\"concatenated_tags\"] + \" \" + df_copy[\"query\"]+ \" \" + df_copy[\"title\"]\n",
    "df_copy = df_copy[df_copy[\"concatenated_tags\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Stop Words\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [mandala, mdf, espirito, santo, mandala, espir...\n",
      "1    [cartao, visita, panfletos, tag, adesivos, cop...\n",
      "2    [expositor, expositor, de, esmaltes, organizad...\n",
      "3    [jogo, lencol, menino, lencol, berco, medidas,...\n",
      "4    [adesivo, box, banheiro, adesivo, box, banheir...\n",
      "5    [albuns, figurinhas, pai, lucas, album, fotos,...\n",
      "6    [mini, arranjos, arranjo, de, flores, para, me...\n",
      "7    [bb, lembrancinhas, maternidade, baby, lembran...\n",
      "8    [dia, pais, chaveiro, dia, dos, pais, chaveiro...\n",
      "9    [nascimento, manta, baby, cha, bebe, vestido, ...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['text']] \n",
    "print(df_copy['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [mandala, mdf, espirito, santo, mandala, espir...\n",
       "1    [cartao, visita, panfleto, tag, adesivo, copo,...\n",
       "2    [expositor, expositor, de, esmalt, organizador...\n",
       "3    [jogo, lencol, menino, lencol, berco, medida, ...\n",
       "4    [adesivo, box, banheiro, adesivo, box, banheir...\n",
       "5    [albun, figurinha, pai, luca, album, foto, dia...\n",
       "6    [mini, arranjo, arranjo, de, flore, para, mesa...\n",
       "7    [bb, lembrancinha, maternidad, babi, lembranca...\n",
       "8    [dia, pai, chaveiro, dia, do, pai, chaveiro, d...\n",
       "9    [nascimento, manta, babi, cha, bebe, vestido, ...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming \n",
    "import nltk.stem.RSLPStemmer()\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "# Get the stemmed_tokens\n",
    "df_copy['stemmed_tokens'] = [[stemmer.stem(word) for word in tokens] for tokens in df_copy['tokenized_text']]\n",
    "df_copy['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words:\n",
      "8566\n",
      "\n",
      "Sample data from dictionary:\n",
      "Word: espirito - ID: 0 \n",
      "Word: mandala - ID: 1 \n",
      "Word: mdf - ID: 2 \n",
      "Word: santo - ID: 3 \n"
     ]
    }
   ],
   "source": [
    "# building dictionaries\n",
    "\n",
    "from gensim import corpora\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df_copy['stemmed_tokens'])\n",
    "print(\"Total unique words:\")\n",
    "print(len(mydict.token2id))\n",
    "print(\"\\nSample data from dictionary:\")\n",
    "i = 0\n",
    "# Print top 4 (word, id) tuples\n",
    "for key in mydict.token2id.keys():\n",
    "    print(\"Word: {} - ID: {} \".format(key, mydict.token2id[key]))\n",
    "    if i == 3:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of how the BOW words\n",
      "Doc2Bow Line:\n",
      "[(0, 2), (1, 2), (2, 1), (3, 2)]\n",
      "Actual line:\n",
      "['mandala', 'mdf', 'espirito', 'santo', 'mandala', 'espirito', 'santo']\n",
      "(Word, count) Tuples:\n",
      "[('espirito', 2), ('mandala', 2), ('mdf', 1), ('santo', 2)]\n",
      "Sparse bow vector for the line\n",
      "[2. 2. 1. ... 0. 0. 0.]\n",
      "Sorted word id list\n",
      "[0, 0, 1, 1, 2, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "#Generating Bow Vectors\n",
    "\n",
    "import gensim\n",
    "vocab_len = len(mydict)\n",
    "print(\"Example of how the BOW words\")\n",
    "arr = []\n",
    "for line in df_copy['stemmed_tokens']:\n",
    "    print(\"Doc2Bow Line:\")\n",
    "    print(mydict.doc2bow(line))\n",
    "    for word in line:\n",
    "        arr.append(mydict.token2id[word])\n",
    "    print(\"Actual line:\")\n",
    "    print(line)\n",
    "    print(\"(Word, count) Tuples:\")\n",
    "    print([(mydict[id], count) for id, count in mydict.doc2bow(line) ])\n",
    "    print(\"Sparse bow vector for the line\")\n",
    "    print(gensim.matutils.corpus2csc([mydict.doc2bow(line)],num_terms=vocab_len).toarray()[:,0])\n",
    "    break\n",
    "print(\"Sorted word id list\")\n",
    "print(sorted(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(8566 unique tokens: ['espirito', 'mandala', 'mdf', 'santo', 'adesivo']...)\n"
     ]
    }
   ],
   "source": [
    "print(mydict)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# separating data into test and train datasets\n",
    "#x_train, x_test, y_train, y_test = train_test_split(\n",
    "#    df_copy[\"text\"], df_copy[\"category\"], test_size=0.25, random_state=0\n",
    "#)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# building pipeline\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#text_clf = Pipeline(\n",
    "#    [\n",
    "#        (\"vect\", CountVectorizer()),\n",
    "#        (\"tfidf\", TfidfTransformer()),\n",
    "#        (\"clf\", MultinomialNB()),\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
