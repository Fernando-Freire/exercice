{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing dataset\n",
    "df_train = pd.read_csv(str( 'sample_products.csv'),sep=',')\n",
    "df_test = pd.read_csv(str( 'test_products.csv'), sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1o passo Removal of Stop Words\n",
    "2o passo Tokenization\n",
    "3o passo Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatening title and tags\n",
    "df_copy = df_train.copy()\n",
    "df_copy[\"text\"] = df_copy[\"concatenated_tags\"] + \" \" + df_copy[\"query\"]+ \" \" + df_copy[\"title\"]\n",
    "df_copy = df_copy[df_copy[\"concatenated_tags\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['text']] \n",
    "print(df_copy['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Stop Words\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "print(\"stop words\")\n",
    "\n",
    "print(len(stopwords))\n",
    "\n",
    "#lista de preposições essenciais\n",
    "prep_essen = ['a', 'ante', 'após', 'até', 'com', 'contra', 'de', 'desde', 'em', 'entre', 'para', 'per',\n",
    "              'perante', 'por', 'sem', 'sob', 'sobre', 'trás']\n",
    "\n",
    "#inclusão de combinações de preposições e artigos\n",
    "\n",
    "\n",
    "prep_essen += ['pelo','ao','pro','do','no','pela','à','pra','da','na',\n",
    "               'pelos','aos','pros','dos','nos','pelas','às','pras','das','nas']\n",
    "\n",
    "#remoção de preposições com mesma composição de artigos e substantivos\n",
    "\n",
    "prep_essen.remove('a')\n",
    "prep_essen.remove('trás')\n",
    "prep_essen.remove('pelo')\n",
    "prep_essen.remove('pelos')\n",
    "\n",
    "\n",
    "#remoção da lista de stopwords as preposições selecionadas\n",
    "\n",
    "\n",
    "for word in prep_essen:\n",
    "    if word in stopwords:\n",
    "        stopwords.remove(word)\n",
    "\n",
    "print(len(stopwords))\n",
    "\n",
    "df_copy['tokens'] = df_copy['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "df_copy['tokens'] = df_copy['tokens'].str.lower()\n",
    "print(df_copy['tokens'].head(10))\n",
    "print(df_copy['tokens'][4])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['tokens']] \n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    split_sentence = sentence.split(' ')\n",
    "    print(split_sentence)\n",
    "    for index in range(0,len(split_sentence)-1):\n",
    "        if split_sentence[index] in prep_essen:\n",
    "            token = split_sentence[index]\n",
    "            token += ' ' + split_sentence[index+1]\n",
    "            split_sentence[index] = token\n",
    "            del split_sentence[index+1]\n",
    "    print(split_sentence)\n",
    "    return split_sentence\n",
    "df_copy['title'] = df_copy['title'].str.lower()\n",
    "\n",
    "df_copy['t_text'] = df_copy['title'].apply(my_tokenizer)\n",
    "print(df_copy['t_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stemming \n",
    "import nltk.stem\n",
    "nltk.download('rslp')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "# Get the stemmed_tokens\n",
    "df_copy['stemmed_tokens'] = [[stemmer.stem(word) for word in tokens] for tokens in df_copy['tokenized_text']]\n",
    "df_copy['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building dictionaries\n",
    "\n",
    "from gensim import corpora\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df_copy['stemmed_tokens'])\n",
    "print(\"Total unique words:\")\n",
    "print(len(mydict.token2id))\n",
    "print(\"\\nSample data from dictionary:\")\n",
    "i = 0\n",
    "# Print top 4 (word, id) tuples\n",
    "for key in mydict.token2id.keys():\n",
    "    print(\"Word: {} - ID: {} \".format(key, mydict.token2id[key]))\n",
    "    if i == 3:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Bow Vectors\n",
    "\n",
    "import gensim\n",
    "vocab_len = len(mydict)\n",
    "print(\"Example of how the BOW words\")\n",
    "arr = []\n",
    "for line in df_copy['stemmed_tokens']:\n",
    "    print(\"Doc2Bow Line:\")\n",
    "    print(mydict.doc2bow(line))\n",
    "    for word in line:\n",
    "        arr.append(mydict.token2id[word])\n",
    "    print(\"Actual line:\")\n",
    "    print(line)\n",
    "    print(\"(Word, count) Tuples:\")\n",
    "    print([(mydict[id], count) for id, count in mydict.doc2bow(line) ])\n",
    "    print(\"Sparse bow vector for the line\")\n",
    "    print(gensim.matutils.corpus2csc([mydict.doc2bow(line)],num_terms=vocab_len).toarray()[:,0])\n",
    "    break\n",
    "print(\"Sorted word id list\")\n",
    "print(sorted(arr))\n",
    "\n",
    "df_copy = df_copy.fillna(0)\n",
    "\n",
    "print(df_copy.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "top_data_df_small = df_copy\n",
    "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['product_id', 'seller_id','search_page','position', 'creation_date', 'price','weight','express_delivery','minimum_quantity','view_counts','order_counts', 'stemmed_tokens']], \n",
    "                                                        top_data_df_small['category'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train set\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test set\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "# Skip-gram model (sg = 1)\n",
    "size = 1000\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "OUTPUT_FOLDER=''\n",
    "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_'  +str(size) + '.model'\n",
    "start_time = time.time()\n",
    "stemmed_tokens = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, workers = workers,vector_size = size, window = window, sg = sg)\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the model from the model file\n",
    "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
    "# Unique ID of the word\n",
    "print(\"Index of the word 'mdf':\")\n",
    "print(sg_w2v_model.wv.key_to_index[\"mdf\"])\n",
    "# Total number of the words \n",
    "print(len(sg_w2v_model.wv))\n",
    "# Print the size of the word2vec vector for one word\n",
    "print(\"Length of the vector generated for a word\")\n",
    "normed_vector = sg_w2v_model.wv.get_vector(\"mdf\", norm=True)\n",
    "print(len(normed_vector))\n",
    "# Get the mean for the vectors for an example review\n",
    "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
    "print(np.mean([len(sg_w2v_model.wv.get_vector(token, norm=True)) for token in top_data_df_small['stemmed_tokens'][0]], axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the vectors for train data in following file\n",
    "OUTPUT_FOLDER =''\n",
    "word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec.csv'\n",
    "with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "    for index, row in X_train.iterrows():\n",
    "        model_vector = (np.mean([sg_w2v_model.wv.get_vector(token, norm=True) for token in row['stemmed_tokens']], axis=0)).tolist()\n",
    "        if index == 0:\n",
    "            ##############ERRO AQUI #############################\n",
    "            header = \",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        # Check if the line exists else it is vector of zeros\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Import the DecisionTreeeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Load from the filename\n",
    "word2vec_df = pd.read_csv(word2vec_filename)\n",
    "#Initialize the model\n",
    "clf_decision_word2vec = DecisionTreeClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "# Fit the model\n",
    "clf_decision_word2vec.fit(word2vec_df, Y_train['category'])\n",
    "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_features_word2vec = []\n",
    "for index, row in X_test.iterrows():\n",
    "    model_vector = np.mean([sg_w2v_model.wv.get_vector(token, norm=True) for token in row['stemmed_tokens']], axis=0)\n",
    "    if type(model_vector) is list:\n",
    "        test_features_word2vec.append(model_vector)\n",
    "    else:\n",
    "        test_features_word2vec.append(np.array([0 for i in range(1000)]))\n",
    "test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
    "print(classification_report(Y_test['category'],test_predictions_word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
