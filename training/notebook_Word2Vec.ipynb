{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing dataset\n",
    "df_train = pd.read_csv(str( 'sample_products.csv'),sep=',')\n",
    "df_test = pd.read_csv(str( 'test_products.csv'), sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1o passo Removal of Stop Words\n",
    "2o passo Tokenization\n",
    "3o passo Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatening title and tags\n",
    "df_copy = df_train.copy()\n",
    "df_copy[\"text\"] = df_copy[\"concatenated_tags\"] + \" \" + df_copy[\"query\"]+ \" \" + df_copy[\"title\"]\n",
    "df_copy = df_copy[df_copy[\"concatenated_tags\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [mandala, mdf, espirito, santo, mandala, espir...\n",
      "1    [cartao, visita, panfletos, tag, adesivos, cop...\n",
      "2    [expositor, expositor, de, esmaltes, organizad...\n",
      "3    [jogo, lencol, menino, lencol, berco, medidas,...\n",
      "4    [adesivo, box, banheiro, adesivo, box, banheir...\n",
      "5    [albuns, figurinhas, pai, lucas, album, fotos,...\n",
      "6    [mini, arranjos, arranjo, de, flores, para, me...\n",
      "7    [bb, lembrancinhas, maternidade, baby, lembran...\n",
      "8    [dia, pais, chaveiro, dia, dos, pais, chaveiro...\n",
      "9    [nascimento, manta, baby, cha, bebe, vestido, ...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['text']] \n",
    "print(df_copy['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    mandala mdf espirito santo Mandala Espírito Santo\n",
      "1    cartao visita panfletos tag adesivos copos lon...\n",
      "2    expositor expositor esmaltes Organizador expos...\n",
      "3    t jogo lencol menino lencol berco medidas lenc...\n",
      "4    adesivo box banheiro adesivo box banheiro ADES...\n",
      "5    albuns figurinhas pai lucas album fotos dia pa...\n",
      "6    mini arranjos arranjo flores mesa Arranjo Flor...\n",
      "7    bb lembrancinhas maternidade baby lembranca ma...\n",
      "8         dia pais chaveiro dia pais chaveiro dia pais\n",
      "9    nascimento manta baby cha bebe vestido bebe ma...\n",
      "Name: tokens, dtype: object\n",
      "0    [mandala, mdf, espirito, santo, mandala, espir...\n",
      "1    [cartao, visita, panfletos, tag, adesivos, cop...\n",
      "2    [expositor, expositor, esmaltes, organizador, ...\n",
      "3    [jogo, lencol, menino, lencol, berco, medidas,...\n",
      "4    [adesivo, box, banheiro, adesivo, box, banheir...\n",
      "5    [albuns, figurinhas, pai, lucas, album, fotos,...\n",
      "6    [mini, arranjos, arranjo, flores, mesa, arranj...\n",
      "7    [bb, lembrancinhas, maternidade, baby, lembran...\n",
      "8    [dia, pais, chaveiro, dia, pais, chaveiro, dia...\n",
      "9    [nascimento, manta, baby, cha, bebe, vestido, ...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removal of Stop Words\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df_copy['tokens'] = df_copy['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "print(df_copy['tokens'].head(10))\n",
    "\n",
    "\n",
    "# tokenization\n",
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['tokens']] \n",
    "print(df_copy['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [mandal, mdf, espirit, sant, mandal, espirit, ...\n",
       "1    [carta, visit, panflet, tag, ades, cop, long, ...\n",
       "2    [exposi, exposi, esmalt, organiz, exposi, esmalt]\n",
       "3    [jog, lencol, menin, lencol, berc, med, lencol...\n",
       "4    [ades, box, banh, ades, box, banh, ades, box, ...\n",
       "5    [album, figur, pai, luc, album, fot, dia, pal,...\n",
       "6    [min, arranj, arranj, fl, mes, arranj, fl, orq...\n",
       "7    [bb, lembranc, matern, baby, lembranc, matern,...\n",
       "8           [dia, pal, chav, dia, pal, chav, dia, pal]\n",
       "9    [nasc, mant, baby, cha, beb, vest, beb, mant, ...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stemming \n",
    "import nltk.stem\n",
    "nltk.download('rslp')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "# Get the stemmed_tokens\n",
    "df_copy['stemmed_tokens'] = [[stemmer.stem(word) for word in tokens] for tokens in df_copy['tokenized_text']]\n",
    "df_copy['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words:\n",
      "6666\n",
      "\n",
      "Sample data from dictionary:\n",
      "Word: espirit - ID: 0 \n",
      "Word: mandal - ID: 1 \n",
      "Word: mdf - ID: 2 \n",
      "Word: sant - ID: 3 \n"
     ]
    }
   ],
   "source": [
    "# building dictionaries\n",
    "\n",
    "from gensim import corpora\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df_copy['stemmed_tokens'])\n",
    "print(\"Total unique words:\")\n",
    "print(len(mydict.token2id))\n",
    "print(\"\\nSample data from dictionary:\")\n",
    "i = 0\n",
    "# Print top 4 (word, id) tuples\n",
    "for key in mydict.token2id.keys():\n",
    "    print(\"Word: {} - ID: {} \".format(key, mydict.token2id[key]))\n",
    "    if i == 3:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of how the BOW words\n",
      "Doc2Bow Line:\n",
      "[(0, 2), (1, 2), (2, 1), (3, 2)]\n",
      "Actual line:\n",
      "['mandal', 'mdf', 'espirit', 'sant', 'mandal', 'espirit', 'sant']\n",
      "(Word, count) Tuples:\n",
      "[('espirit', 2), ('mandal', 2), ('mdf', 1), ('sant', 2)]\n",
      "Sparse bow vector for the line\n",
      "[2. 2. 1. ... 0. 0. 0.]\n",
      "Sorted word id list\n",
      "[0, 0, 1, 1, 2, 3, 3]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37998 entries, 0 to 37999\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   product_id         37998 non-null  int64  \n",
      " 1   seller_id          37998 non-null  int64  \n",
      " 2   query              37998 non-null  object \n",
      " 3   search_page        37998 non-null  int64  \n",
      " 4   position           37998 non-null  int64  \n",
      " 5   title              37998 non-null  object \n",
      " 6   concatenated_tags  37998 non-null  object \n",
      " 7   creation_date      37998 non-null  object \n",
      " 8   price              37998 non-null  float64\n",
      " 9   weight             37998 non-null  float64\n",
      " 10  express_delivery   37998 non-null  int64  \n",
      " 11  minimum_quantity   37998 non-null  int64  \n",
      " 12  view_counts        37998 non-null  int64  \n",
      " 13  order_counts       37998 non-null  float64\n",
      " 14  category           37998 non-null  object \n",
      " 15  text               37998 non-null  object \n",
      " 16  tokenized_text     37998 non-null  object \n",
      " 17  tokens             37998 non-null  object \n",
      " 18  stemmed_tokens     37998 non-null  object \n",
      "dtypes: float64(3), int64(7), object(9)\n",
      "memory usage: 5.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Generating Bow Vectors\n",
    "\n",
    "import gensim\n",
    "vocab_len = len(mydict)\n",
    "print(\"Example of how the BOW words\")\n",
    "arr = []\n",
    "for line in df_copy['stemmed_tokens']:\n",
    "    print(\"Doc2Bow Line:\")\n",
    "    print(mydict.doc2bow(line))\n",
    "    for word in line:\n",
    "        arr.append(mydict.token2id[word])\n",
    "    print(\"Actual line:\")\n",
    "    print(line)\n",
    "    print(\"(Word, count) Tuples:\")\n",
    "    print([(mydict[id], count) for id, count in mydict.doc2bow(line) ])\n",
    "    print(\"Sparse bow vector for the line\")\n",
    "    print(gensim.matutils.corpus2csc([mydict.doc2bow(line)],num_terms=vocab_len).toarray()[:,0])\n",
    "    break\n",
    "print(\"Sorted word id list\")\n",
    "print(sorted(arr))\n",
    "\n",
    "df_copy = df_copy.fillna(0)\n",
    "\n",
    "print(df_copy.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train set\n",
      "Lembrancinhas         12272\n",
      "Decoração              6075\n",
      "Bebê                   4861\n",
      "Papel e Cia            1945\n",
      "Outros                  785\n",
      "Bijuterias e Jóias      660\n",
      "Name: category, dtype: int64\n",
      "Value counts for Test set\n",
      "Lembrancinhas         5252\n",
      "Decoração             2647\n",
      "Bebê                  2069\n",
      "Papel e Cia            805\n",
      "Outros                 347\n",
      "Bijuterias e Jóias     280\n",
      "Name: category, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "   index  product_id  seller_id  search_page  position        creation_date  \\\n",
      "0  29372     5795302    7931459            1         9  2014-01-15 14:59:29   \n",
      "1   7069     8243708    3398473            1        10  2018-06-06 22:26:07   \n",
      "2  28585    13717382    6729875            1        30  2019-05-05 12:24:52   \n",
      "3  36029    14777376    9085143            1        26  2014-02-09 00:41:09   \n",
      "4  11256     5644691    3645206            1        36  2013-10-15 11:07:37   \n",
      "\n",
      "    price  weight  express_delivery  minimum_quantity  view_counts  \\\n",
      "0   14.14    45.0                 1                36          567   \n",
      "1   56.80   705.0                 1                 8          167   \n",
      "2   18.93     6.0                 1                31           70   \n",
      "3   67.42     0.0                 0                 6          157   \n",
      "4  109.10     0.0                 1                 7          179   \n",
      "\n",
      "   order_counts                                     stemmed_tokens  \n",
      "0           3.0  [lembranc, lembranc, cha, beb, lembranc, cha, ...  \n",
      "1           0.0  [almof, person, almof, fot, lembranc, decoraca...  \n",
      "2           0.0  [pao, mel, person, bol, decor, minni, ros, pao...  \n",
      "3           4.0  [lac, faix, beb, ti, acessori, beb, ti, beb, f...  \n",
      "4           0.0             [pass, avuls, pass, barb, tapet, barb]  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "top_data_df_small = df_copy\n",
    "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['product_id', 'seller_id','search_page','position', 'creation_date', 'price','weight','express_delivery','minimum_quantity','view_counts','order_counts', 'stemmed_tokens']], \n",
    "                                                        top_data_df_small['category'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train set\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test set\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 2.092071294784546\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "# Skip-gram model (sg = 1)\n",
    "#size = 1000\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "OUTPUT_FOLDER=''\n",
    "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_'  + '.model'\n",
    "start_time = time.time()\n",
    "stemmed_tokens = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, workers = workers, window = window, sg = sg)\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'mdf':\n",
      "42\n",
      "6666\n",
      "Length of the vector generated for a word\n",
      "1653\n",
      "Print the length after taking average of all word vectors in a sentence:\n",
      "417.2857142857143\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Load the model from the model file\n",
    "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
    "# Unique ID of the word\n",
    "print(\"Index of the word 'mdf':\")\n",
    "print(sg_w2v_model.wv.key_to_index[\"mdf\"])\n",
    "# Total number of the words \n",
    "print(len(sg_w2v_model.wv))\n",
    "# Print the size of the word2vec vector for one word\n",
    "print(\"Length of the vector generated for a word\")\n",
    "print(sg_w2v_model.wv.get_vecattr(\"mdf\", \"count\"))\n",
    "# Get the mean for the vectors for an example review\n",
    "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
    "print(np.mean([sg_w2v_model.wv.get_vecattr(token, \"count\") for token in top_data_df_small['stemmed_tokens'][0]], axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the vectors for train data in following file\n",
    "OUTPUT_FOLDER =''\n",
    "word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec.csv'\n",
    "with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "    for index, row in X_train.iterrows():\n",
    "        model_vector = (np.mean([sg_w2v_model.wv.get_vecattr(token, \"count\") for token in row['stemmed_tokens']], axis=0)).tolist()\n",
    "        if index == 0:\n",
    "            header = \",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        # Check if the line exists else it is vector of zeros\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fit the model with word2vec vectors: 0.0857694149017334\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Import the DecisionTreeeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Load from the filename\n",
    "word2vec_df = pd.read_csv(word2vec_filename)\n",
    "#Initialize the model\n",
    "clf_decision_word2vec = DecisionTreeClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "# Fit the model\n",
    "clf_decision_word2vec.fit(word2vec_df, Y_train['category'])\n",
    "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Bebê       0.00      0.00      0.00      2069\n",
      "Bijuterias e Jóias       0.00      0.00      0.00       280\n",
      "         Decoração       0.00      0.00      0.00      2647\n",
      "     Lembrancinhas       0.46      1.00      0.63      5252\n",
      "            Outros       0.00      0.00      0.00       347\n",
      "       Papel e Cia       0.00      0.00      0.00       805\n",
      "\n",
      "          accuracy                           0.46     11400\n",
      "         macro avg       0.08      0.17      0.11     11400\n",
      "      weighted avg       0.21      0.46      0.29     11400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_features_word2vec = []\n",
    "for index, row in X_test.iterrows():\n",
    "    model_vector = np.mean([sg_w2v_model.wv.get_vecattr(token, \"count\") for token in row['stemmed_tokens']], axis=0)\n",
    "    if type(model_vector) is list:\n",
    "        test_features_word2vec.append(model_vector)\n",
    "    else:\n",
    "        test_features_word2vec.append(np.array([0 for i in range(1000)]))\n",
    "test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
    "print(classification_report(Y_test['category'],test_predictions_word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
