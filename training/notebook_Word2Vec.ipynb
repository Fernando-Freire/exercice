{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing dataset\n",
    "df_train = pd.read_csv(str( 'sample_products.csv'),sep=',')\n",
    "df_test = pd.read_csv(str( 'test_products.csv'), sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1o passo Removal of Stop Words\n",
    "2o passo Tokenization\n",
    "3o passo Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatening title and tags\n",
    "df_copy = df_train.copy()\n",
    "df_copy[\"text\"] = df_copy[\"concatenated_tags\"] + \" \" + df_copy[\"query\"]+ \" \" + df_copy[\"title\"]\n",
    "df_copy = df_copy[df_copy[\"concatenated_tags\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [mandala, mdf, espirito, santo, mandala, espir...\n",
      "1    [cartao, visita, panfletos, tag, adesivos, cop...\n",
      "2    [expositor, expositor, de, esmaltes, organizad...\n",
      "3    [jogo, lencol, menino, lencol, berco, medidas,...\n",
      "4    [adesivo, box, banheiro, adesivo, box, banheir...\n",
      "5    [albuns, figurinhas, pai, lucas, album, fotos,...\n",
      "6    [mini, arranjos, arranjo, de, flores, para, me...\n",
      "7    [bb, lembrancinhas, maternidade, baby, lembran...\n",
      "8    [dia, pais, chaveiro, dia, dos, pais, chaveiro...\n",
      "9    [nascimento, manta, baby, cha, bebe, vestido, ...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['text']] \n",
    "print(df_copy['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words\n",
      "204\n",
      "182\n",
      "0    mandala mdf espirito santo mandala espírito santo\n",
      "1    cartao visita panfletos tag adesivos copos lon...\n",
      "2    expositor expositor de esmaltes organizador ex...\n",
      "3    t jogo lencol menino lencol berco medidas lenc...\n",
      "4    adesivo box banheiro adesivo box banheiro ades...\n",
      "5    albuns figurinhas pai lucas album fotos dia do...\n",
      "6    mini arranjos arranjo de flores para mesa arra...\n",
      "7    bb lembrancinhas maternidade baby lembranca ma...\n",
      "8    dia pais chaveiro dia dos pais chaveiro dia do...\n",
      "9    nascimento manta baby cha bebe vestido bebe ma...\n",
      "Name: tokens, dtype: object\n",
      "adesivo box banheiro adesivo box banheiro adesivo box de banheiro\n"
     ]
    }
   ],
   "source": [
    "# Removal of Stop Words\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "print(\"stop words\")\n",
    "\n",
    "print(len(stopwords))\n",
    "\n",
    "#lista de preposições essenciais\n",
    "prep_essen = ['a', 'ante', 'após', 'até', 'com', 'contra', 'de', 'desde', 'em', 'entre', 'para', 'per',\n",
    "              'perante', 'por', 'sem', 'sob', 'sobre', 'trás']\n",
    "\n",
    "#inclusão de combinações de preposições e artigos\n",
    "\n",
    "\n",
    "prep_essen += ['pelo','ao','pro','do','no','pela','à','pra','da','na',\n",
    "               'pelos','aos','pros','dos','nos','pelas','às','pras','das','nas']\n",
    "\n",
    "#remoção de preposições com mesma composição de artigos e substantivos\n",
    "\n",
    "prep_essen.remove('a')\n",
    "prep_essen.remove('trás')\n",
    "prep_essen.remove('pelo')\n",
    "prep_essen.remove('pelos')\n",
    "\n",
    "\n",
    "#remoção da lista de stopwords as preposições selecionadas\n",
    "\n",
    "\n",
    "for word in prep_essen:\n",
    "    if word in stopwords:\n",
    "        stopwords.remove(word)\n",
    "\n",
    "print(len(stopwords))\n",
    "\n",
    "df_copy['tokens'] = df_copy['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "df_copy['tokens'] = df_copy['tokens'].str.lower()\n",
    "print(df_copy['tokens'].head(10))\n",
    "print(df_copy['tokens'][4])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mandala', 'espírito', 'santo']\n",
      "['mandala', 'espírito', 'santo']\n",
      "['cartão', 'de', 'visita']\n",
      "['cartão', 'de visita']\n",
      "['organizador', 'expositor', 'p/', '70', 'esmaltes']\n",
      "['organizador', 'expositor', 'p/', '70', 'esmaltes']\n",
      "['jogo', 'de', 'lençol', 'berço', 'estampado']\n",
      "['jogo', 'de lençol', 'berço', 'estampado']\n",
      "['adesivo', 'box', 'de', 'banheiro']\n",
      "['adesivo', 'box', 'de banheiro']\n",
      "['álbum', 'de', 'figurinhas', 'dia', 'dos', 'pais']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m split_sentence\n\u001b[1;32m     17\u001b[0m df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 19\u001b[0m df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/series.py:4108\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4107\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 4108\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], Series):\n\u001b[1;32m   4111\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32mpandas/_libs/lib.pyx:2467\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mmy_tokenizer\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(split_sentence)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(split_sentence)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msplit_sentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m prep_essen:\n\u001b[1;32m     11\u001b[0m         token \u001b[38;5;241m=\u001b[39m split_sentence[index]\n\u001b[1;32m     12\u001b[0m         token \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m split_sentence[index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_copy['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df_copy['tokens']] \n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    split_sentence = sentence.split(' ')\n",
    "    print(split_sentence)\n",
    "    for index in range(0,len(split_sentence)-1):\n",
    "        if split_sentence[index] in prep_essen:\n",
    "            token = split_sentence[index]\n",
    "            token += ' ' + split_sentence[index+1]\n",
    "            split_sentence[index] = token\n",
    "            del split_sentence[index+1]\n",
    "    print(split_sentence)\n",
    "    return split_sentence\n",
    "df_copy['title'] = df_copy['title'].str.lower()\n",
    "\n",
    "df_copy['t_text'] = df_copy['title'].apply(my_tokenizer)\n",
    "print(df_copy['t_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [mandal, mdf, espirit, sant, mandal, espirit, ...\n",
       "1    [carta, visit, panflet, tag, ades, cop, long, ...\n",
       "2    [exposi, exposi, esmalt, organiz, exposi, esmalt]\n",
       "3    [jog, lencol, menin, lencol, berc, med, lencol...\n",
       "4    [ades, box, banh, ades, box, banh, ades, box, ...\n",
       "5    [album, figur, pai, luc, album, fot, dia, pal,...\n",
       "6    [min, arranj, arranj, fl, mes, arranj, fl, orq...\n",
       "7    [bb, lembranc, matern, baby, lembranc, matern,...\n",
       "8           [dia, pal, chav, dia, pal, chav, dia, pal]\n",
       "9    [nasc, mant, baby, cha, beb, vest, beb, mant, ...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Stemming \n",
    "import nltk.stem\n",
    "nltk.download('rslp')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "# Get the stemmed_tokens\n",
    "df_copy['stemmed_tokens'] = [[stemmer.stem(word) for word in tokens] for tokens in df_copy['tokenized_text']]\n",
    "df_copy['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words:\n",
      "6666\n",
      "\n",
      "Sample data from dictionary:\n",
      "Word: espirit - ID: 0 \n",
      "Word: mandal - ID: 1 \n",
      "Word: mdf - ID: 2 \n",
      "Word: sant - ID: 3 \n"
     ]
    }
   ],
   "source": [
    "# building dictionaries\n",
    "\n",
    "from gensim import corpora\n",
    "# Build the dictionary\n",
    "mydict = corpora.Dictionary(df_copy['stemmed_tokens'])\n",
    "print(\"Total unique words:\")\n",
    "print(len(mydict.token2id))\n",
    "print(\"\\nSample data from dictionary:\")\n",
    "i = 0\n",
    "# Print top 4 (word, id) tuples\n",
    "for key in mydict.token2id.keys():\n",
    "    print(\"Word: {} - ID: {} \".format(key, mydict.token2id[key]))\n",
    "    if i == 3:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of how the BOW words\n",
      "Doc2Bow Line:\n",
      "[(0, 2), (1, 2), (2, 1), (3, 2)]\n",
      "Actual line:\n",
      "['mandal', 'mdf', 'espirit', 'sant', 'mandal', 'espirit', 'sant']\n",
      "(Word, count) Tuples:\n",
      "[('espirit', 2), ('mandal', 2), ('mdf', 1), ('sant', 2)]\n",
      "Sparse bow vector for the line\n",
      "[2. 2. 1. ... 0. 0. 0.]\n",
      "Sorted word id list\n",
      "[0, 0, 1, 1, 2, 3, 3]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37998 entries, 0 to 37999\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   product_id         37998 non-null  int64  \n",
      " 1   seller_id          37998 non-null  int64  \n",
      " 2   query              37998 non-null  object \n",
      " 3   search_page        37998 non-null  int64  \n",
      " 4   position           37998 non-null  int64  \n",
      " 5   title              37998 non-null  object \n",
      " 6   concatenated_tags  37998 non-null  object \n",
      " 7   creation_date      37998 non-null  object \n",
      " 8   price              37998 non-null  float64\n",
      " 9   weight             37998 non-null  float64\n",
      " 10  express_delivery   37998 non-null  int64  \n",
      " 11  minimum_quantity   37998 non-null  int64  \n",
      " 12  view_counts        37998 non-null  int64  \n",
      " 13  order_counts       37998 non-null  float64\n",
      " 14  category           37998 non-null  object \n",
      " 15  text               37998 non-null  object \n",
      " 16  tokenized_text     37998 non-null  object \n",
      " 17  tokens             37998 non-null  object \n",
      " 18  stemmed_tokens     37998 non-null  object \n",
      "dtypes: float64(3), int64(7), object(9)\n",
      "memory usage: 5.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Generating Bow Vectors\n",
    "\n",
    "import gensim\n",
    "vocab_len = len(mydict)\n",
    "print(\"Example of how the BOW words\")\n",
    "arr = []\n",
    "for line in df_copy['stemmed_tokens']:\n",
    "    print(\"Doc2Bow Line:\")\n",
    "    print(mydict.doc2bow(line))\n",
    "    for word in line:\n",
    "        arr.append(mydict.token2id[word])\n",
    "    print(\"Actual line:\")\n",
    "    print(line)\n",
    "    print(\"(Word, count) Tuples:\")\n",
    "    print([(mydict[id], count) for id, count in mydict.doc2bow(line) ])\n",
    "    print(\"Sparse bow vector for the line\")\n",
    "    print(gensim.matutils.corpus2csc([mydict.doc2bow(line)],num_terms=vocab_len).toarray()[:,0])\n",
    "    break\n",
    "print(\"Sorted word id list\")\n",
    "print(sorted(arr))\n",
    "\n",
    "df_copy = df_copy.fillna(0)\n",
    "\n",
    "print(df_copy.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train set\n",
      "Lembrancinhas         12272\n",
      "Decoração              6075\n",
      "Bebê                   4861\n",
      "Papel e Cia            1945\n",
      "Outros                  785\n",
      "Bijuterias e Jóias      660\n",
      "Name: category, dtype: int64\n",
      "Value counts for Test set\n",
      "Lembrancinhas         5252\n",
      "Decoração             2647\n",
      "Bebê                  2069\n",
      "Papel e Cia            805\n",
      "Outros                 347\n",
      "Bijuterias e Jóias     280\n",
      "Name: category, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "   index  product_id  seller_id  search_page  position        creation_date  \\\n",
      "0  29372     5795302    7931459            1         9  2014-01-15 14:59:29   \n",
      "1   7069     8243708    3398473            1        10  2018-06-06 22:26:07   \n",
      "2  28585    13717382    6729875            1        30  2019-05-05 12:24:52   \n",
      "3  36029    14777376    9085143            1        26  2014-02-09 00:41:09   \n",
      "4  11256     5644691    3645206            1        36  2013-10-15 11:07:37   \n",
      "\n",
      "    price  weight  express_delivery  minimum_quantity  view_counts  \\\n",
      "0   14.14    45.0                 1                36          567   \n",
      "1   56.80   705.0                 1                 8          167   \n",
      "2   18.93     6.0                 1                31           70   \n",
      "3   67.42     0.0                 0                 6          157   \n",
      "4  109.10     0.0                 1                 7          179   \n",
      "\n",
      "   order_counts                                     stemmed_tokens  \n",
      "0           3.0  [lembranc, lembranc, cha, beb, lembranc, cha, ...  \n",
      "1           0.0  [almof, person, almof, fot, lembranc, decoraca...  \n",
      "2           0.0  [pao, mel, person, bol, decor, minni, ros, pao...  \n",
      "3           4.0  [lac, faix, beb, ti, acessori, beb, ti, beb, f...  \n",
      "4           0.0             [pass, avuls, pass, barb, tapet, barb]  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split Function\n",
    "top_data_df_small = df_copy\n",
    "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['product_id', 'seller_id','search_page','position', 'creation_date', 'price','weight','express_delivery','minimum_quantity','view_counts','order_counts', 'stemmed_tokens']], \n",
    "                                                        top_data_df_small['category'], \n",
    "                                                        shuffle=shuffle_state,\n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=15)\n",
    "    print(\"Value counts for Train set\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Test set\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train))\n",
    "    print(type(Y_train))\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    Y_train = Y_train.to_frame()\n",
    "    Y_train = Y_train.reset_index()\n",
    "    Y_test = Y_test.to_frame()\n",
    "    Y_test = Y_test.reset_index()\n",
    "    print(X_train.head())\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 6.769005537033081\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "# Skip-gram model (sg = 1)\n",
    "size = 1000\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "OUTPUT_FOLDER=''\n",
    "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_'  +str(size) + '.model'\n",
    "start_time = time.time()\n",
    "stemmed_tokens = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, workers = workers,vector_size = size, window = window, sg = sg)\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'mdf':\n",
      "42\n",
      "6666\n",
      "Length of the vector generated for a word\n",
      "1000\n",
      "Print the length after taking average of all word vectors in a sentence:\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Load the model from the model file\n",
    "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
    "# Unique ID of the word\n",
    "print(\"Index of the word 'mdf':\")\n",
    "print(sg_w2v_model.wv.key_to_index[\"mdf\"])\n",
    "# Total number of the words \n",
    "print(len(sg_w2v_model.wv))\n",
    "# Print the size of the word2vec vector for one word\n",
    "print(\"Length of the vector generated for a word\")\n",
    "normed_vector = sg_w2v_model.wv.get_vector(\"mdf\", norm=True)\n",
    "print(len(normed_vector))\n",
    "# Get the mean for the vectors for an example review\n",
    "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
    "print(np.mean([len(sg_w2v_model.wv.get_vector(token, norm=True)) for token in top_data_df_small['stemmed_tokens'][0]], axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the vectors for train data in following file\n",
    "OUTPUT_FOLDER =''\n",
    "word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec.csv'\n",
    "with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "    for index, row in X_train.iterrows():\n",
    "        model_vector = (np.mean([sg_w2v_model.wv.get_vector(token, norm=True) for token in row['stemmed_tokens']], axis=0)).tolist()\n",
    "        if index == 0:\n",
    "            ##############ERRO AQUI #############################\n",
    "            header = \",\".join(str(ele) for ele in range(1000))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "        # Check if the line exists else it is vector of zeros\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(1000)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Import the DecisionTreeeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Load from the filename\n",
    "word2vec_df = pd.read_csv(word2vec_filename)\n",
    "#Initialize the model\n",
    "clf_decision_word2vec = DecisionTreeClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "# Fit the model\n",
    "clf_decision_word2vec.fit(word2vec_df, Y_train['category'])\n",
    "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_features_word2vec = []\n",
    "for index, row in X_test.iterrows():\n",
    "    model_vector = np.mean([sg_w2v_model.wv.get_vector(token, norm=True) for token in row['stemmed_tokens']], axis=0)\n",
    "    if type(model_vector) is list:\n",
    "        test_features_word2vec.append(model_vector)\n",
    "    else:\n",
    "        test_features_word2vec.append(np.array([0 for i in range(1000)]))\n",
    "test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
    "print(classification_report(Y_test['category'],test_predictions_word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
